{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dictionary with some sample data\n",
    "data = {'date': [\n",
    "'2018-01-01', '2018-02-01',\n",
    "'2018-03-01', '2018-04-01',\n",
    "'2018-05-01', '2018-06-01',\n",
    "'2018-01-01', '2018-02-01',\n",
    "'2018-03-01', '2018-04-01',\n",
    "'2018-05-01', '2018-06-01'],\n",
    "'visitors': [35, 30, 82, 26, 83, 46, 40, 57, 95, 57, 87, 42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['date', 'visitors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when looking at the dataset, the rows have been given a number (starting with 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *visitors* column is of interger type, but the date column is shown to be an object. We know that this is a date and it would be preferable to use a more relevant type.\n",
    "We can change this column with to_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use to to_datetime method to convert Pandas columns into date object\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# We set index and sort the dataframe by that index\n",
    "df.set_index('date', inplace=True)\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The inplace property lets us make changes direclty to the dataframe. Otherwise, we would need to make copies of it to appy the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all data points\n",
    "df['2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are filtering for the visitors in May\n",
    "df['2018-05']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other slicing and dicing techniques used in collection objects are possible thanks to the use of the colon notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[datetime(2018, 3, 1):]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *truncate* methods can help us keep all the data points before or after a given data. In this case, let us ask for the data up to March 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can truncate the time series with the method\n",
    "df.truncate(after=datetime(2018, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.truncate(after=datetime(2018, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.truncate(before=datetime(2018, 5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can calculate aggregations with the help of groupby. In this case we are interested in the count\n",
    "df.groupby('date').count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we have two entries for each date. We can also look at statistics such as the mean and the sum of entries.\n",
    "In this case, we are going to use `resample` method for a series.  \n",
    "In effect this enables us to change the time frequency in our dataset. Let us use the `M` *offset alias* to tell Pandas to create\n",
    "monthly statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample('M').mean() # we calculate the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample('M').sum() # we calculate the sum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Alias | Description |\n",
    "| ----- | ----------- |\n",
    "| B | business day frequency |\n",
    "| C | custom bunisness day frequency |\n",
    "| D | calendar day frequency |\n",
    "| W | weekly frequency |\n",
    "| M | month-end frequency |\n",
    "| Q | quarter-end frequency |\n",
    "| H | hourly frequency |\n",
    "| S | secondly frequency |\n",
    "| N | nanoseconds\n",
    "| BQ | business quarter-end frequency |\n",
    "| QS | quarter start frequency |\n",
    "| SM | semi-month-end frequency (15th and end of month) |\n",
    "| BM | business month-end frequency |\n",
    "| MS | month-start frequency |\n",
    "| BH | business hour frequency |\n",
    "| BQS | business quarter start frequency |\n",
    "| SMS | semi-month-start frequency (1st and 15th) |\n",
    "| CBM | custom business month-end frequency |\n",
    "| BMS | business month start frequency |\n",
    "| CBMS | custom business month-start frequency |\n",
    "| A, Y | year-end frequency |\n",
    "| L, ms | milliseconds |\n",
    "| U, us | microseconds |\n",
    "| BA, By | business year-end frequency |\n",
    "| AS, YS | year-start frequency |\n",
    "| T, min | minutely frequency |\n",
    "| BAS, BYS | businness year-start frequency |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for data entered manually. We are not including the count in this table\n",
    "df.groupby('date').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can provide a data in plain natural language, and convert it to a date type\n",
    "date = pd.to_datetime(\"14th of October, 2016\")\n",
    "date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Directive | Meaning |\n",
    "| --------- | ------- |\n",
    "| %a | abbreviated weekday name |\n",
    "| %A | full weekday name |\n",
    "| %b | abbreviated month name |\n",
    "| %B | full month name |\n",
    "| %c | preferred date and time representation |\n",
    "| %d | day of the month (1 to 31) |\n",
    "| %D | same as %m/%d/%y\n",
    "| %e | day of the month (1 to 31) |\n",
    "| %m | month (1 to 12) |\n",
    "| %M | minute |\n",
    "| %S | second |\n",
    "| %u | weekday as number (Mon = 1 to 7) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%A => ' + date.strftime('%A'))\n",
    "print('%a => ' +  date.strftime('%a'))\n",
    "print('%b => ' +  date.strftime('%b'))\n",
    "print('%B => ' +  date.strftime('%B'))\n",
    "print('%c => ' +  date.strftime('%c'))\n",
    "print('%d => ' +  date.strftime('%d'))\n",
    "print('%D => ' +  date.strftime('%D'))\n",
    "print('%e => ' +  date.strftime('%e'))\n",
    "print('%m => ' +  date.strftime('%m'))\n",
    "print('%M => ' +  date.strftime('%M'))\n",
    "print('%S => ' +  date.strftime('%S'))\n",
    "print('%U => ' +  date.strftime('%U'))\n",
    "print('%u => ' +  date.strftime('%u'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some case we may need to create time series data from scratch. In this section we are going to explore some of the ways  \n",
    "which pandas enables us to crate and manipulate time series data on top the commands we have discussed up until this point.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can determine a time series by specifying start and end times\n",
    "pd.date_range('2018-05-30', '2018-06-02')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output of the command above is an index covering the time range requested with a daily frequency  \n",
    "as shown in the output with `freq=\"D\"`  \n",
    "\n",
    "An alternative to the above command is to provide a start date, but instead of giving an end date, we request a  \n",
    "number of `periods` to cover with the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2023-02-28', periods=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we provide a start time a number of periods and the frequency for those periods\n",
    "# as you can see, all we had to do was specify the monthly frequency with freq=\"M\"\n",
    "pd.date_range('2023-02-28', periods=4, freq=\"M\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us construct a more complicated dataset: For a period of four days starting on June 4, 2018; we take  \n",
    "reading for four features called, **A, B, C e D**. In this case we will generate the readings with a random  \n",
    "number sampled from a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "idx = pd.date_range('2018-06-04 00:00:00', periods=4)\n",
    "cols = ['A', 'B', 'C', 'D']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create data for four rows and four columns with help of randin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randn(m, n) creates an anrray of m row and columns\n",
    "data = randn(len(idx), len(cols))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we used random numbers to generate the dta, the numbers shown here will differ from\n",
    "# those you may obtain on your computer\n",
    "\n",
    "df = pd.DataFrame(data=data, index=idx, columns=cols)\n",
    "df.index.name = 'date'\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A table like the one above is useful to summarise data and it is fit for `human consumption`. However, in many application  \n",
    "it is much better to have a `long format`or `melted` dataset  \n",
    "\n",
    "In ordet to achieve this, we nedd to repeat the dates and we also require a new column to hold the feature to which each  \n",
    "rading corresponds. This can easily be done with **Pandas** in an single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is because we need to date to be part of the new formatted dataset\n",
    "df.reset_index(inplace=True)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to melt the dataframe, we will use the melt method that takes the following parameters:  \n",
    "A column that will become the new identifier variable with `id_vars`, the columns to un-pivot are  \n",
    "specified with `value_vars` and finally the names for the variables and value columns with `var_name` \n",
    "and `value_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original columns have become entries in the column called 'feature' and the values are in column 'reading'\n",
    "melted = pd.melt(df, id_vars='date', var_name='feature', value_name='reading')\n",
    "melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted.set_index('date', inplace=True)\n",
    "melted.sort_index(inplace=True)\n",
    "melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_files = os.path.join(os.getcwd(), '..', 'data', 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that you pass on the correct path for the file\n",
    "appl = pd.read_csv(os.path.join(base_files, 'advanced-data-science-aapl.csv'), sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(appl.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using to_datetime to ensure that ates are appropriately typed\n",
    "appl.Date = pd.to_datetime(appl.Date, format='%Y-%m-%d')\n",
    "type(appl.Date)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains open, high, low and close prices for Apple Inc. stock between April 2017 and April 2018  \n",
    "We are goint to concentrate on the `Close` column, but before we do that, we need to ensure tat the dataset is  \n",
    "`indexed` by the time stamps provided by the `Date column`. We can easily do that with the `set_index method`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We centre our attention on the use of the closing prices\n",
    "appl.set_index('Date', inplace=True)\n",
    "appl['Close'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appl.sort_index(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "R_{t} = \\frac{P_{t} - P_{t} -1}{P_{t} -1}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $ P_{t} $ is the price \n",
    "* *t*  is the time\n",
    "* $ P_{t} - 1 $ is the price at the previous time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using pct_change to calculate the returns\n",
    "appl['pct_change'] = appl.Close.pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The percentage change from one day to the next is easily calculated\n",
    "appl['pct_change'].tail(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ r_{t} = log(1 + R_{t}) = log(\\frac{P_{t}}{P_{t} -1}) = log(P_{t}) - log(P_{t} - 1) * (1.2) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate the logarithm of the price at each time *t* and then take the difference between time periods.  \n",
    "We can certainly do this in Python, and Pandas gives us a helping hand with the diff()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The diff method calculate the difference from one time period to the next\n",
    "appl['log_ret'] = np.log(appl.Close).diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appl['log_ret'].tail(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the data that we show, we calculated that time series in the figure. It is fairly common to have  \n",
    "financial data series like the one we have used above, where the frequency is given by the end of day prices.  \n",
    "However, the frequency can be different for instance by the minimun upward or downward price movement in the price  \n",
    "of security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = lambda date: pd.datetime.strptime(date, '%d/%m/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin = pd.read_csv(os.path.join(base_files, 'bitcoin_usd.csv'), parse_dates=['time_start'], date_parser=parser, index_col='time_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we are specifiying what columns need to be parsed as dates with **parse_dates** and how  \n",
    "the parsing should be performed with **date_parser**. We also load the dataset indicating which column  \n",
    "is the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = bitcoin[['close', 'volume']] # we are effectily creating a new dataframe called ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is roughly on a minute-by-minute frequency. We can use Pandas\n",
    "# to resample the data at desired intervals. For instance we can request for the data\n",
    "# to be sampled every five minute and take the first value in the interval.\n",
    "\n",
    "ticks.resample('5Min').first() # we can resample our data with the help of resample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also ask for the mean for example\n",
    "ticks.resample('5Min').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this way we could get the closing price for the day by resampling by day\n",
    "# and requesting the last volume\n",
    "ticks.resample('D').last()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we know how to resample the data, we can consider creating a new open\n",
    "# high, low and close set of prices for the resampled data.\n",
    "# \n",
    "# The ohlc() method lets us find the OHLC prices for our new sampled data\n",
    "bars = ticks['close'].resample('5Min').ohlc()\n",
    "bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas will take the first and last values in the interval to be open\n",
    "# and close for the bar. Then it will take the max and min as the high and low\n",
    "# respectively. In this way, we start filtering the data. For example, imagine we are\n",
    "# interested in the price between 10 am and 4 pm each day\n",
    "filtered = bars.between_time('10:00', '16:00')\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may be interested in looking at the price first thing in the morning\n",
    "# in this case, we are using at_time method\n",
    "bars.open.at_time('8:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not only that, we can request the percentage change too by combining the methods we have\n",
    "# already discussed\n",
    "bars.open.at_time('8:00').pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note that the first percentage change connot be calculated as we do not have\n",
    "# a comparison data point from the previous interval. In this case, pandas indicates this by\n",
    "# the use of NaN\n",
    "bars.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can fill in missing data with help of fillna, which taks a parameter called method.\n",
    "# It can be either 'pad' or 'ffill' to propagate last valid observation forward or instead\n",
    "# 'backfill' or 'bfill' to use the next valid observation to fill the gap.\n",
    "#\n",
    "# Here we have filled the missing data by bringing the last value forward and limiting the operation to one time period\n",
    "bars.fillna(method='ffill', limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filledbars = bars.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = ticks.volume.resample('5Min').sum()\n",
    "vol = volume.fillna(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filledbars['2016-04-03'].between_time('9:00', '23:59').plot(color=['gray', 'gray', 'gray', 'k'], style=['-', '--', '-.', '-+'])\n",
    "vol['2016-04-03'].between_time('9:30','23:59').plot(secondary_y=True, style='k-o')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".virtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec  7 2022, 13:47:07) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba8d7f511d8bc67dfdde966ff5fb1ec5dc3f82972ade794bfe454c3efce046b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
